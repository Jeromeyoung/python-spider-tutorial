# 爬虫实在案例

## 实战目标

- 采集最近一日世界各国疫情数据
- 采集1/23 以来世界各国的疫情数据
- 采集最近一日全国各省疫情数据
- 采集1/23 以来全国各省疫情数据
- 对整体项目进行重构

## 案例1：采集近一日各国疫情数据

```python
import requests
from bs4 import BeautifulSoup
import json
import re

# 1. 下载疫情首页源码
url = "https://ncov.dxy.cn/ncovh5/view/pneumonia"
response = requests.get(url)
home_page = response.content.decode("utf-8")

# 2. 解析网页源码为文档树结构，然后通过标题和内容其他需要的内容；
soup = BeautifulSoup(home_page, 'lxml')
script_tag = soup.find(id="getListByCountryTypeService2true")
strings = script_tag.string

# 3. 使用re模块进行字符串匹配提取json字符串（re.findall 获取到的是列表，我们要的字符串）
json_str_list = re.findall(r"\[.+\]", strings)
json_str = json_str_list[0]

# 4. 将json字符串数据转换为 python数据对象；
last_day_cornna_virus = json.loads(json_str)
print(type(last_day_cornna_virus))

# 5. 将python对象以json格式存储起来；(注意编码格式)
with open(r"./data/last_day_cornna_virus.json", 'w', encoding='utf-8') as fp:
    json.dump(last_day_cornna_virus, fp, ensure_ascii=False)

```



## 案例1：代码重构...

- 重构代码，以提高代码的可扩展性
  - （1）把功能封装到一个类当中；
  - （2）把每一个功能组合一个方法；
  - （3）通过run方法启动爬虫；

```python
import requests
from bs4 import BeautifulSoup
import json
import re


class CoronaVirusSpider:

    def __init__(self):
        self.url = "https://ncov.dxy.cn/ncovh5/view/pneumonia"

    def get_content_from_url(self):
        """
        获取指定标签中的指定内容字符串
        :param: 请求的url
        :return: 响应内容字符串
        """
        response = requests.get(self.url)
        home_page = response.content.decode("utf-8")
        return home_page

    def parse_home_page(self, home_page):
        """
        解析网页源码，通过指定标签和字符串内容获取json字符，然后转换获取python数据
        :param: home_page 首页内容
        :return: 解析转换后的python数据
        """
        # 2. 解析网页源码为文档树结构，然后通过标签和内容其他需要的内容；
        soup = BeautifulSoup(home_page, 'lxml')
        script_tag = soup.find(id="getListByCountryTypeService2true")
        strings = script_tag.string

        # 3. 使用re模块进行字符串匹配提取json字符串（re.findall 获取到的是列表，我们要的字符串）
        json_str_list = re.findall("\[.+\]", strings)
        json_str = json_str_list[0]

        # 4. 将json字符串数据转换为 python数据对象；
        data = json.loads(json_str)
        return data

    def save_to_json(self, data, path):
        """
        5. 将python对象以json格式存储起来；
        :param:
        :return:
        """
        with open(path, 'w', encoding='utf-8') as fp:
            json.dump(data, fp, ensure_ascii=False)

    def crawl_last_day_corona_virus(self):
        """
        采集最近一天各国疫情的数据
        :return:
        """
        # 1. 发送请求，获取首页内容
        home_page = self.get_content_from_url()
        # 2. 解析首页内容，获取最近一天疫情各国数据
        last_day_corona_virus = self.parse_home_page(home_page)
        # 3. 保存疫情json数据
        self.save_to_json(last_day_corona_virus, r"./data/last_dat_corona_virus.json")

    def run(self):
        self.crawl_last_day_corona_virus()


if __name__ == "__main__":
    spider = CoronaVirusSpider()
    spider.run()
```

## 案例2：

- 理解2

> - （1）加载json文档疫情数据，将其转换为python字典数据；
> - （2）通过字典关键字，循环提取各国1/23以来疫情数据的url;  关键字：statisticsData
> - （3）直接获取每个url的源码内容：get_content_from_url()
> - （4）将json字符串数据转换为Python可操作数据对象；
> - 为什么不能直接存储为json文档，原因是因为这里的数据没有国家信息，需要先转换为python字典对象，以方便添加“国家”关键字信息；
> - （5）最后保持为json文档；



- 理解 1

> 总体目标：下载1月23以来各国每日疫情数据
>
> （1）加载json文件为python可操控对象(字典)；
> （2）通过关键字提取url链接；
> （3）通过链接逐个现在各个1月23日以来每日疫情json数据
> （4）转换json数据为python可操作对象；
> （5）添加“国家”关键字信息，以方便后续的识别和调用；
> （6）最后，在将python数据保证为josn文件存储起来；



## 案例3

### 学习理解

- version 1.0

> 1. 下载首页源码
> 2. 解析首页源码，依据id = "xxx"提取所需疫情数据
> 3. 保障疫情数据为json文件；

### 实战源码

```python
import requests
from bs4 import BeautifulSoup
import re
import json
from tqdm import tqdm


class CoronaVirusSpider:
    def __init__(self):
        self.home_url = "https://ncov.dxy.cn/ncovh5/view/pneumonia"

    def get_content_from_url(self, url):
        """
         1. 下载疫情首页源码
        :param url: home_url
        :return: home_page
        """
        response = requests.get(url)
        home_page = response.content.decode()
        return home_page

    def parse_home_page(self, home_page, tag_id):
        """
        解析home_page,提取script标签中的json字符，并返回python数据对象
        :param home_page: 疫情数据首页源码
        :param tag_id: 疫情数据 tag_id ="getListByCountryTypeService2true"
        :return data: 返回python数据对象
        """
        # 2. 解析疫情首页源码为文档树结构，使用标签和内容提取疫情数据
        soup = BeautifulSoup(home_page, 'lxml')
        script_tag = soup.find(id=tag_id)
        script_string = script_tag.string

        # 3. 使用正则，匹配疫情数据，提取出json字符串数据（json_str）
        json_str_list = re.findall(r"\[.+\]",script_string)
        json_str = json_str_list[0]

        # 4. 将json字符串数据转换为 python数据类型
        data = json.loads(json_str)
        return data

    def save_to_json(self, file_path, data):
        """
        将疫情python数据保存为json文件
        :param file_path: data/corona_virus.json
        :param data: 疫情数据(python字典）
        """
        with open(file_path, 'w', encoding='utf-8') as fp:
            json.dump(data, fp, ensure_ascii=False)

    def crawl_last_day_corona_virus(self):
        # 1.发送源码下载请求，并返回源码
        home_page = self.get_content_from_url(self.home_url)
        # 2. 提取引起字符串数据，并转为python对象
        data = self.parse_home_page(home_page, "getListByCountryTypeService2true")
        # 3. 保证python数据为json文件
        self.save_to_json(r"data/last_day_corona_virus.json", data)

    def crawl_corona_virus(self):
        # （1）加载json文件为python可操控对象；
        with open(r'data/last_day_corona_virus.json', 'r', encoding='utf-8') as fp:
            last_day_corona_virus = json.load(fp)

        # （2）通过关键字提取url链接；
        for country in tqdm(last_day_corona_virus, "采集1月23日后的每日疫情数据"):
            country_url = country["statisticsData"]
            # print(country_url)

            # （3）通过链接逐个现在各个1月23日以来每日疫情json数据
            country_json_str = self.get_content_from_url(country_url)

            # （4）转换json数据为python可操作对象,并提取所需要疫情数据；
            country_dict = json.loads(country_json_str)
            data_of_country_dict = country_dict['data']
            # print(data_of_country_dict)

            # （5）添加“国家”关键字信息到每日数据记录中，以方便后续识别和调用；
            for everyday in data_of_country_dict:
                everyday["provinceName"] = country["provinceName"]
                everyday["countryShortCode"] = country["countryShortCode"]

        # （6）最后，在将python数据保证为json文件存储起来；
        self.save_to_json('data/corona_virus.json')

    def crawl_corona_virus_of_china(self):
        # 1. 下载首页源码
        home_page = self.get_content_from_url(self.home_url)
        # 2. 解析所需要的疫情数据
        data = self.parse_home_page(home_page, tag_id="getAreaStat")
        # 3. 保证为json文件
        self.save_to_json(r"data/last_day_corona_virus_of_china.json", data)

    def run(self):
        # self.crawl_last_day_corona_virus()
        # self.crawl_corona_virus()
        self.crawl_corona_virus_of_china()

if __name__ == "__main__":
    spider = CoronaVirusSpider()
    spider.run()
```



